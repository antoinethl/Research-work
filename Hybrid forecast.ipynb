{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.rinterface\n",
    "\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "base = importr('base')\n",
    "Factoextra = importr(\"factoextra\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class representing the time series and every information needed to work with.\n",
    "Contains functions to discretize it, to find the optimal number of clusters, the optimal time step and the models window size.\n",
    "Contains the models used for the hybrid prediction, functions to calculate and adjust the weights that correspond to the importance of the models for the prediction of the final values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeries():\n",
    "    \n",
    "    \"\"\"\n",
    "        A class representing the time series and every information needed to work with it\n",
    "        Contains functions to discretize it, finds the optimal number of clusters, the optimal \n",
    "        time step and the models window size\n",
    "        Contains the models used for the hybrid prediction, functions to calculate and adjust\n",
    "        the weights that correspond to the importance of the models for the prediction of the final values\n",
    "        \n",
    "        @Attributes:\n",
    "        conf: dict\n",
    "            The configuration of the time series where every useful information is stored \n",
    "            like the optimal number of cluster, the time step, the device code, \n",
    "            the optimal window size for each model...\n",
    "        df: DataFrame\n",
    "            The DataFrame obtained after reading the data files\n",
    "        ts: array of float\n",
    "            The raw time series\n",
    "        ts_dis: array of float\n",
    "            The discretized time series\n",
    "        trainX: array of float\n",
    "            The training inputs\n",
    "        trainY: array of float\n",
    "            The training labels/output\n",
    "        testX: array of float\n",
    "            The testing inputs\n",
    "        testY:\n",
    "            The testing labels/output\n",
    "        models: dict\n",
    "            Dictionary containing the models used for predicting the time series, keys are the models names\n",
    "        predict: DataFrame\n",
    "            DataFrame that contains every prediction for each model and for each window in the test set\n",
    "            Contains the final prediction computed with the weights\n",
    "        error: DataFrame\n",
    "            DataFrame that contains the MAE computed between the prediction of each model and the original test set\n",
    "        weights: array of float\n",
    "            The weights of the models that correspond to their importance for the prediction of the final values \n",
    "        last_values: DataFrame\n",
    "            Last values used by the weights calculation function for the standard deviation \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, device_code=None, time_step=None, load_fileconf=None, load_dictconf=None, \n",
    "                 nbr_of_files=1000, threshold=15):\n",
    "        \n",
    "        \"\"\"\n",
    "            @Params:\n",
    "            device_code: str\n",
    "                The device code for the appliance to study, \n",
    "                useless if arguments load_fileconf or load_dictconf aren't None\n",
    "            time_step: int\n",
    "                The time step to use when reading the files, represents a time step in minutes, \n",
    "                if left None, the time step will be computed automatically\n",
    "            load_fileconf: str\n",
    "                The filename to load a configuration from a json stored locally in a text file\n",
    "            load_dictconf: dict\n",
    "                Load a configuration from a dictionary\n",
    "            nbr_of_files: int\n",
    "                The number of files to read\n",
    "            threshold: int\n",
    "                Threshold representing the limit while computing the MAPE \n",
    "                between raw time series and scaled time series\n",
    "        \"\"\"\n",
    "        \n",
    "        if load_fileconf:\n",
    "            self.load_conf(load_fileconf)\n",
    "          \n",
    "        elif load_dictconf:\n",
    "            self.conf = load_dictconf\n",
    "            \n",
    "        else:   \n",
    "            self.conf = {\n",
    "                \"device\": device_code\n",
    "            }\n",
    "        \n",
    "            if time_step:\n",
    "                self.conf[\"time_step\"] = time_step  \n",
    "                \n",
    "            else:\n",
    "                self.conf[\"time_step\"] = find_time_step(device_code, threshold=threshold)\n",
    "            \n",
    "        self.df = create_dataframe(self.conf[\"device\"], self.conf[\"time_step\"], nbr_of_files=nbr_of_files)\n",
    "        self.df = self.df.dropna()\n",
    "        self.ts = self.df[\"active_power\"]\n",
    "        \n",
    "    \n",
    "    def set_nb_clusters(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Finds the optimal number of clusters needed to discretize the time series\n",
    "            Uses the fviz_nbclust() from Factoextra in an embedded R code to compute it\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Finding the optimal number of clusters...\")\n",
    " \n",
    "        sample = ro.r.matrix(self.df[self.df[\"filename\"].between(1, 4)][\"active_power\"].to_numpy())\n",
    "                \n",
    "        r=ro.r(\"\"\"\n",
    "            check = function(matrix) {\n",
    "            n_clust = fviz_nbclust(matrix, kmeans, k.max = 15)\n",
    "\n",
    "            n_clust = n_clust$data\n",
    "\n",
    "            max_cluster = as.numeric(n_clust$clusters[which.max(n_clust$y)])\n",
    "            return(max_cluster)\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "        result = r(sample)\n",
    "        self.conf[\"nb_clust\"] = int(result[0])\n",
    "        \n",
    "        print(f\"Optimal number of clusters is {self.conf['nb_clust']}\\n\")\n",
    "                    \n",
    "            \n",
    "    def discretize(self, threshold=100):\n",
    "        \n",
    "        \"\"\"\n",
    "            Discretizes the time series using the number of clusters in the configuration dictionary\n",
    "            Computes the mape between the original time series and the discretized one\n",
    "            If the mape is too high, the orginal time series will be used instead of the discretized one \n",
    "            \n",
    "            @Params:\n",
    "            threshold: int\n",
    "                The maximum threshold not to be exceeded for the discretization\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Starting discretization...\")\n",
    "        \n",
    "        if not \"nb_clust\" in self.conf.keys():\n",
    "            print(\"No number of clusters assigned\\n\")\n",
    "            self.set_nb_clusters()\n",
    "        \n",
    "        self.ts_dis = clustering(self.ts, self.conf[\"nb_clust\"])\n",
    "        \n",
    "        error = MAPE(self.ts, self.ts_dis)\n",
    "        print(error)\n",
    "        \n",
    "        if error > threshold:\n",
    "            self.ts_dis = self.ts\n",
    "            print(\"\\nError is too high, original time series will be used instead of disctretized one\\n\")\n",
    "            \n",
    "        print(\"Discretization done\\n\")\n",
    "        \n",
    "         \n",
    "    def plot_series(self, t1=0, t2=100, t1p=None, t2p=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            Plots the raw time series and the discretized one\n",
    "            \n",
    "            @Params:\n",
    "            t1: int\n",
    "                The lower bound index when plotting the time series\n",
    "            t2: int\n",
    "                The upper bound index when plotting the time series\n",
    "            t1p: float between 0 and 1\n",
    "                The lower bound index in percentage when plotting \n",
    "            t2p: float between 0 and 1\n",
    "                The upper bound of time in percentage when plotting\n",
    "        \"\"\"\n",
    "        \n",
    "        plot_discretized(self.ts, self.ts_dis, t1=t1, t2=t2, t1p=t1p, t2p=t2p)\n",
    "        \n",
    "        \n",
    "    def find_w_size(self, name, output_width=30, min_w=10, max_w=75, step=5, offset=0, \n",
    "                    sum_=False, window=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            Finds the best input window size for a given model using an interval of values\n",
    "            Computes the loss over windows size and keeps the size with the minimum error on the validation set\n",
    "            \n",
    "            @Params:\n",
    "            name: str\n",
    "                The name of the model used for the prediction, allowed names are 'CNN_LSTM', 'CNN', 'LSTM' and 'MLP'\n",
    "            output_width: int\n",
    "                The output window size\n",
    "            min_w: int\n",
    "                The minimum value of the interval to find the best window size\n",
    "            max_w: int\n",
    "                The maximum value of the interval to find the best window size\n",
    "            step: int\n",
    "                The step of values inside the interval\n",
    "            offset: int\n",
    "                The offset between the last value of the input window and the first value of the output window\n",
    "            sum_: bool\n",
    "                If true, the output will be the sum over the next 30 minutes\n",
    "            window: int\n",
    "                Optional input window size, if used, the function won't find the best window size and will use \n",
    "                this value instead\n",
    "                \n",
    "            @Returns:\n",
    "            window_size: int\n",
    "                The optimal window size for the given model\n",
    "        \"\"\"\n",
    "        \n",
    "        loss_arr = []\n",
    "        val_loss_arr = []\n",
    "        \n",
    "        print(\"Finding best window size..\")\n",
    "        print(f\"Model: {name}, output size: {output_width}\\n\")\n",
    "        \n",
    "        if window:\n",
    "            \n",
    "            self.create_train_test(name=name, f_size=window, offset=offset, output_width=output_width, sum_=sum_)\n",
    "            model, loss, val_loss = get_model(name, self.trainX, self.trainY)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(min_w, max_w, step):\n",
    "            \n",
    "                self.create_train_test(name=name, f_size=i, offset=offset, output_width=output_width, sum_=sum_)\n",
    "                model, loss, val_loss = get_model(name, self.trainX, self.trainY)\n",
    "                \n",
    "                print(f\"For window of {i} values, MAPE = {loss}\")\n",
    "                loss_arr.append(loss)\n",
    "                val_loss_arr.append(val_loss)\n",
    "                \n",
    "                temp = np.insert(val_loss_arr, 0, val_loss_arr[0])\n",
    "                temp = np.append(temp, val_loss_arr[-1])\n",
    "            \n",
    "                smooth = np.convolve(temp, [1, 2, 1], mode='valid')\n",
    "     \n",
    "                if (len(smooth)-np.argmin(smooth)) > 4:\n",
    "                    break\n",
    "                \n",
    "            print(\"Done\")\n",
    "            \n",
    "            val_loss_arr = np.insert(val_loss_arr, 0, val_loss_arr[0])\n",
    "            val_loss_arr = np.append(val_loss_arr, val_loss_arr[-1])\n",
    "            val_loss_arr_smooth = np.convolve(val_loss_arr, [1, 2, 1], mode='valid')     \n",
    "            \n",
    "            idx = np.argmin(val_loss_arr_smooth)\n",
    "            \n",
    "            window_size = range(min_w, max_w, step)[idx]\n",
    "            \n",
    "            range_ = range(min_w, max_w, step)[:len(loss_arr)]\n",
    "            plt.plot(range_, loss_arr, label=\"loss\", color=\"#33638DFF\")\n",
    "            plt.plot(range_, val_loss_arr[1:-1], label=\"val_loss\", color=\"#3CBB75FF\")\n",
    "            plt.plot(range_, val_loss_arr_smooth/4, \n",
    "                     label=\"smooth_val_loss\", color=\"#d18756\")\n",
    "            \n",
    "            plt.axvline(x=window_size, linestyle=\"--\", c=\"black\", lw=1)\n",
    "            plt.legend()\n",
    "            plt.title(name + \" model\")\n",
    "            plt.xlabel(\"window size\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Best window size for {name} is {window_size}\\n\")\n",
    "\n",
    "            return window_size\n",
    "    \n",
    "    \n",
    "    def create_train_test(self, name, f_size=10, offset=0, output_width=1, train_size=0.8, sum_=False):\n",
    "        \n",
    "        \"\"\"\n",
    "            Creates a train and a test set for the deep learning models\n",
    "            \n",
    "            @Params:\n",
    "            name: str\n",
    "                The name of the model used for the prediction, allowed names are 'CNN_LSTM', 'CNN', 'LSTM' and 'MLP'\n",
    "            f_size: int\n",
    "                The input window size\n",
    "            offset: int\n",
    "                The offset between the last value of the input window and the first value of the output window\n",
    "            output_width:\n",
    "                The output window size\n",
    "            train_size: float between 0 and 1\n",
    "                The proportion of the dataset to include in the train set\n",
    "            sum_: bool\n",
    "                If true, the output will be the sum over the next 30 minutes   \n",
    "        \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'ts_dis'):\n",
    "            print(\"Time series isn't discretized\\n\")\n",
    "            self.discretize()\n",
    "        \n",
    "        n_train = int(train_size * len(self.ts_dis))\n",
    "\n",
    "        if \"CNN\" in name:\n",
    "            train = self.ts[:n_train]\n",
    "            test = self.ts[n_train:]\n",
    "        \n",
    "        else:\n",
    "            train = self.ts_dis[:n_train]\n",
    "            test = self.ts_dis[n_train:]\n",
    "            \n",
    "        self.trainX, self.trainY = create_X_Y(train, f_size, offset, output_width, sum_)\n",
    "        self.testX, self.testY = create_X_Y(test, f_size, offset, output_width, sum_)\n",
    "        \n",
    "        \n",
    "    def get_models_size(self, models=[\"CNN_LSTM\", \"CNN\", \"LSTM\", \"MLP\"], size=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            Finds the optimal input window size for each model, stores the results in the conf dictionary\n",
    "            \n",
    "            @Params:\n",
    "            models: array of string\n",
    "                The names of the models used for the prediction, allowed names are 'CNN_LSTM', 'CNN', 'LSTM' and 'MLP'\n",
    "            size: dict\n",
    "                Optional dictionary containing the size for each model, keys correspond to the models names,\n",
    "                if left None, the algorithm will find the optimal window size\n",
    "        \"\"\"\n",
    "                \n",
    "        self.conf[\"w_sizes\"] = {}\n",
    "        \n",
    "        for model in models:\n",
    "            \n",
    "            print(f\"Searching optimal window size for {model}:\\n\")\n",
    "            \n",
    "            if size:\n",
    "                self.conf[\"w_sizes\"][model] = size[model]\n",
    "                \n",
    "            else:\n",
    "                self.conf[\"w_sizes\"][model] = self.find_w_size(model, output_width=int(30/self.conf[\"time_step\"]))\n",
    "              \n",
    "            \n",
    "    def get_models(self, offset=0, sum_=False):\n",
    "        \n",
    "        \"\"\"\n",
    "            Creates and stores the models that will be used for predicting the time series\n",
    "            according to their kind and their input window stored in the configuration dictionary\n",
    "            Stores the models in a new dictionary\n",
    "            For each model and for each predicted window, store the prediction in a new DataFrame named \"predict\"\n",
    "            \n",
    "            @Params:\n",
    "            offset: int\n",
    "                The offset between the last value of the input window and the first value of the output window\n",
    "            sum_: bool\n",
    "                If true, the output will be the sum over the next 30 minutes\n",
    "        \"\"\"\n",
    "            \n",
    "        self.models = {}\n",
    "        self.predict = pd.DataFrame()\n",
    "        min_value = min(self.conf[\"w_sizes\"].values())\n",
    "           \n",
    "        output_width = int(30/self.conf[\"time_step\"])\n",
    "        \n",
    "        \n",
    "        for name in self.conf[\"w_sizes\"].keys():\n",
    "                \n",
    "            size = self.conf[\"w_sizes\"][name]\n",
    "            self.create_train_test(name=name, f_size=size, offset=offset, output_width=output_width, sum_=sum_)\n",
    "            model, loss, val_loss = get_model(name, self.trainX, self.trainY)\n",
    "            \n",
    "            pred = pd.DataFrame({name: model.predict(self.testX).tolist()},\n",
    "                                index=range(size-min_value, len(self.testY)+(size-min_value)))\n",
    "            \n",
    "            pred[name] = pred[name].apply(lambda x: np.array(x))\n",
    "            \n",
    "            self.predict = pd.concat([self.predict, pred], axis=1)\n",
    "                \n",
    "            self.models[name] = model\n",
    "            \n",
    "            del model, pred\n",
    "            \n",
    "        self.create_train_test(name=\"CNN\", f_size=min_value, offset=offset, output_width=output_width, sum_=sum_)\n",
    "        self.predict[\"test\"] = self.testY.tolist()\n",
    "        self.create_train_test(name=\"MLP\", f_size=min_value, offset=offset, output_width=output_width, sum_=sum_)\n",
    "        self.predict[\"test_dis\"] = self.testY.tolist()\n",
    "        \n",
    "        self.predict.dropna(inplace=True)\n",
    "        \n",
    "    \n",
    "    def compute_error(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Computes the error between each model prediction for each window in the test set, stores the result\n",
    "            in a new DataFrame. After this, creates a new column in the \"predict\" DataFrame with all errors in\n",
    "            a numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        self.error = pd.DataFrame()\n",
    "        \n",
    "        for name in self.conf[\"w_sizes\"].keys():\n",
    "            \n",
    "            self.error[f\"mae {name}\"] = self.predict[[name, \"test\"]].apply(lambda x: mae(x), axis=1)\n",
    "            self.error[f\"mape {name}\"] = self.predict[[name, \"test\"]].apply(lambda x: MAPE(x[0], x[1]), axis=1)\n",
    "            \n",
    "        self.predict['error'] = self.error.filter(like='mae').apply(lambda r: tuple(r), axis=1).apply(np.array)\n",
    "        \n",
    "        \n",
    "    def compute_weights_init(self, size_max=30, learning_rate=0.1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initializes the weights calculation and applies the function \"compute_weights\" \n",
    "            for each prediction in the \"predict\" DataFrame\n",
    "            Stores the result in a new column in the \"predict\" DataFrame\n",
    "            Computes the final time series with the model weights and their predictions\n",
    "            Finally, computes the final error\n",
    "            \n",
    "            @Params:\n",
    "            size_max: int\n",
    "                The maximum size of the array containing the last errors to calculate the standard deviation\n",
    "            learning_rate: float\n",
    "                The value by which the new weights will be multiplied, it determines how\n",
    "                newly acquired information will override old information, a value close to 1 \n",
    "                will only consider new information while 0 will prevent it to learn\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = [0] * len(self.models)\n",
    "        self.last_values = pd.DataFrame()\n",
    "        \n",
    "        self.predict[\"weights\"] = self.predict['error'].apply(lambda x: self.compute_weights(x))\n",
    "        \n",
    "        self.predict[\"final\"] = self.predict[[*self.models.keys()]].apply(lambda x: self.compute_final_values(x), axis=1)\n",
    "        \n",
    "        self.error[\"mae final\"] = self.predict[[\"final\", \"test\"]].apply(lambda x: mae(x), axis=1)\n",
    "        self.error[\"mape final\"] = self.predict[[\"final\", \"test\"]].apply(lambda x: MAPE(x[0], x[1]), axis=1)\n",
    "        \n",
    "    \n",
    "    def compute_weights(self, error, size_max=30, learning_rate=0.1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Computes the weight that each model will have for the prediction, a model which makes smaller errors and\n",
    "            has a smaller standard deviation will have more weight for the prediction of the final values\n",
    "            \n",
    "            @Params:\n",
    "            error: array of float\n",
    "                The model errors used to update the weights\n",
    "            size_max: int\n",
    "                The maximum size of the array containing the last errors to calculate the standard deviation\n",
    "            learning_rate: float\n",
    "                The value by which the new weights will be multiplied, it determines how\n",
    "                newly acquired information will override old information, a value close to 1 \n",
    "                will only consider new information while 0 will prevent it to learn\n",
    "                \n",
    "            @Returns:\n",
    "            weights: array of float\n",
    "                The new updated weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.last_values = self.last_values.append([error], ignore_index=True)\n",
    "        \n",
    "        if len(self.last_values) > size_max:\n",
    "            self.last_values = self.last_values.drop(self.last_values.index[0]).reset_index(drop=True)\n",
    "    \n",
    "        nw = error\n",
    "\n",
    "        nw = [(max(nw)-elem+min(nw)) / sum(nw) for elem in nw]\n",
    "        nw = [elem / sum(nw) for elem in nw]\n",
    "    \n",
    "        if np.array(self.weights).any():\n",
    "            \n",
    "            std = self.last_values.std().values\n",
    "            std = [(max(std)-elem+min(std)) / sum(std) for elem in std]\n",
    "\n",
    "            weights = np.add(self.weights, [learning_rate*a*b for a,b in zip(nw, std)])\n",
    "            weights = [round((elem / sum(weights)), 2) for elem in weights]\n",
    "            self.weights = weights\n",
    "            \n",
    "        else:\n",
    "            self.weights = nw\n",
    "            \n",
    "        return np.array(self.weights)\n",
    "    \n",
    "    \n",
    "    def compute_final_values(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "            Computes the final values with the model weights and their predictions\n",
    "            \n",
    "            @Params:\n",
    "            x: Slice of DataFrame\n",
    "                The DataFrame columns that contains the predictions of the models \n",
    "                \n",
    "            @Returns:\n",
    "            values: array of float\n",
    "                The final window computed with the model weights and their predictions\n",
    "        \"\"\"\n",
    "            \n",
    "        values = np.zeros(len(x[0]))\n",
    "    \n",
    "        for i in range(len(x)):\n",
    "            values = values + np.array(x.values[i] * self.weights[i])\n",
    "        \n",
    "        return values\n",
    "    \n",
    "    \n",
    "    def plot_predictions(self, names=None, min_=1, max_=1000):\n",
    "        \n",
    "        \"\"\"\n",
    "            Plots the predicted windows for a given interval, some windows are skipped to avoid overlaps\n",
    "            1 window displayed for 30/time_step predicted\n",
    "            \n",
    "            @Params:\n",
    "            names: array of string\n",
    "                If left None, will display every predictions + the original time series, \"names\" can be used\n",
    "                to choose the predictions that must be plotted\n",
    "            min_: int\n",
    "                The lower bound index when plotting the time series\n",
    "            max_: int\n",
    "                The upper bound index when plotting the time series\n",
    "        \"\"\"\n",
    "        \n",
    "        if not names:\n",
    "            names = [*self.models.keys()] + [\"test\", \"final\"]\n",
    "\n",
    "        arr = range(min_, max_, int(30/self.conf[\"time_step\"]))\n",
    "\n",
    "        plt.figure(figsize=(16, 7), dpi=75)\n",
    "\n",
    "        for name in names:\n",
    "            plt.plot(np.concatenate(self.predict.iloc[arr][name].to_numpy()), label=name)\n",
    "\n",
    "        plt.title(\"Predictions\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def plot_weights(self,):\n",
    "        \n",
    "        \"\"\"\n",
    "            Plots the weights evolution for each model used for the predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        weights_evolution = pd.DataFrame(self.predict[\"weights\"].values.tolist(), columns=[*self.models.keys()])\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        for name in weights_evolution.columns:\n",
    "            plt.plot(weights_evolution[name], label=name)\n",
    "\n",
    "        plt.title(\"Weights evolution\")\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "        del weights_evolution\n",
    "        \n",
    "\n",
    "    def save_conf(self, name=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            Saves a configuration as a json into a text file\n",
    "            \n",
    "            @Params:\n",
    "            name: str\n",
    "                The name that will be given to the file, if left None, the filename will be given according\n",
    "                to the current date and the device code\n",
    "        \"\"\"\n",
    "        \n",
    "        if name:\n",
    "            filename = name\n",
    "            \n",
    "        else:\n",
    "            filename = \"conf_\" + str(self.conf[\"device\"]) + \"_\" + datetime.today().strftime('%Y-%m-%d') + \".txt\"\n",
    "            \n",
    "        with open(filename, \"w\") as file:\n",
    "            json.dump(self.conf, file)\n",
    "    \n",
    "            \n",
    "    def load_conf(self, filename):\n",
    "        \n",
    "        \"\"\"\n",
    "            Loads a configuration from a json stored in a text file\n",
    "            \n",
    "            @Params:\n",
    "            filename: str\n",
    "                The filename to load the configuration\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(filename) as file:\n",
    "            self.conf = json.loads(file.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains the main function to read files and load time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe(device_type=None, time=0, nbr_of_files=10000, show=True):\n",
    "    \n",
    "    \"\"\"\n",
    "        Reads the Excel files and create a pandas DataFrame containing \n",
    "        the time series information for a given device\n",
    "        \n",
    "        @Params:\n",
    "        device_type: str\n",
    "            String containing \"0+int\" between 0 and 8, which represent the code for the device type to be\n",
    "            studied, if None, the loop will read every files for a complete dataset\n",
    "        time: int \n",
    "            The timescale to use when reading the files, represents a time step in minutes\n",
    "        nbr_of_files: int\n",
    "            The number of files to be read\n",
    "        show: bool\n",
    "            If True, will display the informations while reading the files\n",
    "            \n",
    "        @Returns:\n",
    "        df: DataFrame\n",
    "            The DataFrame containing the informations from files for the given device\n",
    "    \"\"\"\n",
    "    \n",
    "    path = \"../\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    i = 1\n",
    "\n",
    "    for root, directories, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if (filename.split(\".\")[-1] == \"gzip\") and (filename[0:2] == device_type or device_type == None):\n",
    "\n",
    "                df2 = pd.read_parquet(os.path.join(root, filename)) \n",
    "                \n",
    "                df2[\"date\"] = df2[\"timestamp\"].apply(lambda x: datetime.fromtimestamp(x/1000))\n",
    "                \n",
    "                if time != 0:   \n",
    "                    df2 = df2.set_index('date').resample(f'{time}T').mean().reset_index()\n",
    "                \n",
    "                df2[\"filename\"] = i\n",
    "                df2[\"device\"] = filename\n",
    "                df = df.append(df2)\n",
    "                if show: print(\"File {}, {} | {} rows\".format(i, filename, len(df2)))\n",
    "                i += 1\n",
    "                \n",
    "        if i > nbr_of_files:\n",
    "            break\n",
    "            \n",
    "    df = df.drop_duplicates(subset=['date'])\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error helping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to compute or print the errors between time series / predicted windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "        Computes the MAPE between 2 time series\n",
    "        \n",
    "        @Params:\n",
    "        y_true: array of float\n",
    "            The original time series\n",
    "        y_pred: array of float\n",
    "            The predicted, interpolated or discretized time series\n",
    "            \n",
    "        @Returns:\n",
    "        float\n",
    "            The MAPE between the 2 time series\n",
    "    \"\"\"\n",
    "    \n",
    "    m = MeanAbsolutePercentageError()\n",
    "    m.update_state(y_true, y_pred)\n",
    "    \n",
    "    return m.result().numpy()\n",
    "\n",
    "\n",
    "def print_error(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "        Prints the MSE, the MAE and the MAPE between 2 time series\n",
    "        \n",
    "        @Params:\n",
    "        y_true: array of float\n",
    "            The original time series\n",
    "        y_pred: array of float\n",
    "            The predicted, interpolated or discretized time series\n",
    "    \"\"\"\n",
    "     \n",
    "    print(\"\\tMAPE: \" + str(MAPE(y_true, y_pred)))\n",
    "    print(\"\\tMSE: \" + str(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"\\tMAE: \" + str(mean_absolute_error(y_true, y_pred)))\n",
    "\n",
    "    \n",
    "def mae(x):\n",
    "    \n",
    "    \"\"\"\n",
    "        Computes and returns the MAE between a prediction window and the original window\n",
    "            \n",
    "        @Params:\n",
    "        x: object\n",
    "            Slice of a DataFrame with 2 columns, the one containing every predicted windows and the other with\n",
    "            the original windows\n",
    "            \n",
    "        @Returns:\n",
    "        float\n",
    "            The MAE between the 2 value windows\n",
    "    \"\"\"\n",
    "\n",
    "    return mean_absolute_error(x[0], x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time step selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are used to find the optimal time step for a given time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_timescale(df, kind=\"linear\", graph=True, error=True):\n",
    "    \n",
    "    \"\"\"\n",
    "        Checks if the right timescale has been selected, computes the MAPE between the raw time series and\n",
    "        a time series resampled at a given time step, interpolated to have the same length than the raw one\n",
    "        \n",
    "        @Params:\n",
    "        df: DataFrame\n",
    "            The DataFrame containing the time series information\n",
    "        kind: str\n",
    "            Kind of interpolation, default is linear but can be cubic..      \n",
    "        graph: bool\n",
    "            If True, plots the different graphs\n",
    "        error: bool\n",
    "            If True, prints the error\n",
    "            \n",
    "        @Returns\n",
    "        float\n",
    "            The MAPE between the raw time series and the interpolated one\n",
    "    \"\"\"\n",
    "    \n",
    "    raw = create_dataframe(device_type=df['device'].iloc[0][0:2], time=1, nbr_of_files=1, show=False)\n",
    "    \n",
    "    x = df[df[\"filename\"].between(1, 4)].index\n",
    "    y = df.iloc[x][\"active_power\"].to_numpy()\n",
    "    \n",
    "    step = len(raw) / len(x)\n",
    "    x = (x*step).astype(int)\n",
    "    \n",
    "    f = interp1d(x, y, kind=kind)\n",
    "    xnew = range(max(x))\n",
    "\n",
    "    new = f(xnew)\n",
    "    \n",
    "    if graph:\n",
    "        plt.scatter(x=range(0, 240, 2), y=y[:120], s=8, color=\"#4e3185\")\n",
    "        plt.title(\"Re-sampled time series\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(x=range(0, 240, 2), y=y[:120], s=8, color=\"#4e3185\")\n",
    "        plt.plot(new[:240], color=\"#5071ad\", label=\"Interpolated\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Interpolated time series\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(raw[\"active_power\"][:240], color=\"#731135\", label=\"Original\")\n",
    "        plt.scatter(x=range(0, 240, 2), y=y[:120], s=8, color=\"#4e3185\")\n",
    "        plt.plot(new[:240], color=\"#5071ad\", label=\"Interpolated\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Re-sampled vs original\")\n",
    "        plt.show()\n",
    "\n",
    "    if error: \n",
    "        print_error(raw[\"active_power\"][0:max(x)], new)\n",
    "    \n",
    "    return MAPE(raw[\"active_power\"][0:max(x)], new)\n",
    "    del raw\n",
    "\n",
    "\n",
    "def find_time_step(device_code, threshold=15):\n",
    "    \n",
    "    \"\"\"\n",
    "        Checks multiple time steps to find the optimal one, while the error (MAPE) is smaller than the threshold, \n",
    "        the time step to resample the time series increases. Time steps used are 1, 2, 5, 10, 15 minutes\n",
    "        \n",
    "        @Params:\n",
    "        device_code: str\n",
    "            String containing \"0+int\" between 0 and 8, which represents the code for the device type to be studied\n",
    "        threshold: int\n",
    "            The maximum value that the MAPE mustn't cross\n",
    "            \n",
    "        @Returns\n",
    "        time_step: int\n",
    "            The optimal computed time step\n",
    "    \"\"\"\n",
    "    \n",
    "    time_step = 1\n",
    "    \n",
    "    for i in [1, 2, 5, 10, 15]:\n",
    "        \n",
    "        print(f\"Checking error with {i} min...\")\n",
    "        df = create_dataframe(device_code, i, 1, show=False)\n",
    "        error = check_timescale(df, graph=False, error=False)\n",
    "        print(f\"\\tMAPE : {error}\\n\")\n",
    "        \n",
    "        if error < threshold:\n",
    "            time_step = i\n",
    "        #else:\n",
    "            #break\n",
    "            \n",
    "    print(f\"Optimal time step is {time_step} min\\n\\n\")  \n",
    "    \n",
    "    return time_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering and the discretization functions that will help to discretize the time series and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretize_signal(labels, centroids):\n",
    "    \n",
    "    \"\"\"\n",
    "        Discretizes a time series using the labels computed with a clustering method and the centroids\n",
    "        \n",
    "        @Params:  \n",
    "        labels: array of int\n",
    "            The labels found with the clustering method\n",
    "        centroids: array of float\n",
    "            The centroid for each cluster\n",
    "            \n",
    "        @Returns:\n",
    "        ts: array of float\n",
    "            The discretized time series\n",
    "    \"\"\"\n",
    "    \n",
    "    function = lambda x: centroids[x]\n",
    "    ts = np.array([function(xi) for xi in labels]).reshape(-1)\n",
    "    \n",
    "    return ts\n",
    "\n",
    "\n",
    "def plot_discretized(raw, discretized, t1=0, t2=100, t1p=None, t2p=None):\n",
    "    \n",
    "    \"\"\"\n",
    "        Plots the discretized time series compared to the raw one\n",
    "        \n",
    "        @Params:\n",
    "        raw: array of float\n",
    "            The raw time series\n",
    "        discretized: array of float \n",
    "            The discretized time series\n",
    "        t1: int\n",
    "            The lower bound index when plotting the time series\n",
    "        t2: int\n",
    "            The upper bound index when plotting the time series\n",
    "        t1p: float between 0 and 1\n",
    "            The lower bound index in percentage when plotting \n",
    "        t2p: float between 0 and 1\n",
    "            The upper bound of time in percentage when plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    if t1p and t2p:\n",
    "        t1 = int(len(raw)*(t1p/100))\n",
    "        t2 = int(len(raw)*(t2p/100))\n",
    "    \n",
    "    plt.plot(raw[t1:t2], color=\"#731135\")\n",
    "    plt.title(\"Original time series\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(discretized[t1:t2], color=\"#5071ad\")\n",
    "    plt.title(\"Discretized time series\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(raw[t1:t2], label=\"Original\", color=\"#731135\")\n",
    "    plt.plot(discretized[t1:t2], label=\"Discretized\", color=\"#5071ad\")\n",
    "    plt.title(\"Discretization\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def clustering(x, n_clusters=3, error=True, plot=True):\n",
    "    \n",
    "    \"\"\"\n",
    "        Applies K-means clustering method to the time series according to the number of clusters given\n",
    "        \n",
    "        @Params:\n",
    "        df: DataFrame\n",
    "            The DataFrame containing the time series information\n",
    "        n_clusters: int\n",
    "            The number of clusters\n",
    "        error: bool\n",
    "            If True, prints the error\n",
    "        plot: bool\n",
    "            If True, plots the different graphs\n",
    "            \n",
    "        @Returns:\n",
    "        ts: array of float\n",
    "            The discretized time series\n",
    "    \"\"\"\n",
    "\n",
    "    X = x.to_numpy().reshape(-1, 1)\n",
    "\n",
    "    clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "\n",
    "    ts = discretize_signal(clustering.labels_, clustering.cluster_centers_)\n",
    "\n",
    "    if plot: plot_discretized(X, ts, t1=0, t2=200)\n",
    "    if error: print_error(X, ts)\n",
    "    \n",
    "    return ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that format and shape the time series into a dataset with input windows and output windows that can be used by Deep Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_X_Y(ts, f_size=10, offset=0, output_width=1, sum_=False):\n",
    "    \n",
    "    \"\"\"\n",
    "        From a given time series, creates an input array containing the input windows and a label array containing \n",
    "        the output windows for machine/deep learning tasks\n",
    "        \n",
    "        @Params:\n",
    "        ts: array of float\n",
    "            The time series\n",
    "        f_size: int\n",
    "            The input window size\n",
    "        offset: int\n",
    "            The offset between the last value of the input window and the first value of the output window\n",
    "        output_width:\n",
    "            The output window size\n",
    "        sum_: bool\n",
    "            If true, the label to predict will be the sum of the values contained in the output window\n",
    "            \n",
    "        @Returns:\n",
    "        X: array of float\n",
    "            The input array containing the input windows\n",
    "        Y: array of float\n",
    "            The label array containing the output windows\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(ts) - f_size - offset - output_width):\n",
    "        if (output_width == 1):\n",
    "            Y.append(ts[i + f_size + offset])\n",
    "        elif not sum_:\n",
    "            Y.append(ts[i + f_size + offset:i + f_size + offset + output_width])\n",
    "        else:\n",
    "            Y.append(np.sum(ts[i + f_size + offset:i + f_size + offset + output_width]))\n",
    "        X.append(ts[i:(i + f_size)])\n",
    "    \n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    \n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "    if sum_: Y = np.reshape(Y, (Y.shape[0], 1))\n",
    "\n",
    "    return X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains every Deep Learning models used for the hybrid prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(name, trainX, trainY):\n",
    "    \n",
    "    if name == \"CNN_LSTM\":\n",
    "        model, loss, val_loss = getLSTM_CNN(trainX, trainY)\n",
    "        \n",
    "    elif name == \"CNN\":\n",
    "        model, loss, val_loss = getCNN(trainX, trainY)\n",
    "        \n",
    "    elif name == \"LSTM\":\n",
    "        model, loss, val_loss = getLSTM(trainX, trainY)\n",
    "        \n",
    "    else:\n",
    "        model, loss, val_loss = getMLP(trainX, trainY)\n",
    "        \n",
    "    return model, loss, val_loss\n",
    "        \n",
    "\n",
    "def getLSTM_CNN(trainX, trainY):\n",
    "        \n",
    "    model = Sequential([\n",
    "        \n",
    "        Conv1D(filters=60, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "        \n",
    "        LSTM(48, return_sequences=True),\n",
    "        LSTM(48, return_sequences=False),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        Dense(trainY.shape[1], kernel_initializer=tf.initializers.zeros),\n",
    "        Lambda(lambda x: x * 400)\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    \n",
    "    history = model.fit(trainX, trainY, epochs=15, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    return model, np.array(history.history[\"loss\"][-3:]).mean(), np.array(history.history[\"val_loss\"][-3:]).mean()\n",
    "\n",
    "\n",
    "def getCNN(trainX, trainY):\n",
    "    \n",
    "    model = Sequential([\n",
    "        \n",
    "        Conv1D(filters=64, kernel_size=3, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(trainY.shape[1], kernel_initializer=tf.initializers.zeros),\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    history = model.fit(trainX, trainY, epochs=25, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    return model, np.array(history.history[\"loss\"][-3:]).mean(), np.array(history.history[\"val_loss\"][-3:]).mean()\n",
    "    \n",
    "    \n",
    "def getMLP(trainX, trainY):\n",
    "    \n",
    "    model = Sequential([\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(30, activation='relu'),\n",
    "        Dense(30, activation='relu'),\n",
    "        Dense(trainY.shape[1], kernel_initializer=tf.initializers.zeros),\n",
    "        \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    history = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    return model, np.array(history.history[\"loss\"][-3:]).mean(), np.array(history.history[\"val_loss\"][-3:]).mean()\n",
    "\n",
    "\n",
    "def getLSTM(trainX, trainY):\n",
    "\n",
    "    model = Sequential([\n",
    "        \n",
    "        LSTM(48, activation='relu', return_sequences=True),\n",
    "        LSTM(48, activation='relu', return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(trainY.shape[1], kernel_initializer=tf.initializers.zeros),\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.003)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "\n",
    "    history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_split=0.2)\n",
    "    \n",
    "    return model, np.array(history.history[\"loss\"][-3:]).mean(), np.array(history.history[\"val_loss\"][-3:]).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries(\"05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.discretize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.get_models_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.compute_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.compute_weights_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.error.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.save_conf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
